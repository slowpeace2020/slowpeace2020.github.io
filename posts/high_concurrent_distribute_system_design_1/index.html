<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>High_concurrent_distribute_system_design | slowpeace2020-blog</title>
<meta name=keywords content="system design,distribute,concurrent"><meta name=description content="High_concurrent_distribute_system_design introduction"><meta name=author content="Slowpeace"><link rel=canonical href=https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_1/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as=style><link rel=icon href=https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="High_concurrent_distribute_system_design"><meta property="og:description" content="High_concurrent_distribute_system_design introduction"><meta property="og:type" content="article"><meta property="og:url" content="https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_1/"><meta property="og:image" content="https://slowpeace2020.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-19T08:46:46-04:00"><meta property="article:modified_time" content="2024-05-19T08:46:46-04:00"><meta property="og:site_name" content="slowpeace2020-blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://slowpeace2020.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="High_concurrent_distribute_system_design"><meta name=twitter:description content="High_concurrent_distribute_system_design introduction"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://slowpeace2020.github.io/posts/"},{"@type":"ListItem","position":2,"name":"High_concurrent_distribute_system_design","item":"https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"High_concurrent_distribute_system_design","name":"High_concurrent_distribute_system_design","description":"High_concurrent_distribute_system_design introduction","keywords":["system design","distribute","concurrent"],"articleBody":"演进篇之数据库 池化技术 刚开始数据库的调用方式是先获取数据库的连接，然后依靠这条连接从数据库中查询数据，最后关闭连接释放数据库资源。频繁地建立数据库连接耗费时间长导致了访问慢的问题。整个 MySQL 的连接过程大概消耗了 4ms，SQL 的平均执行时间大概是 1ms，也就是说相比于 SQL 的执行，MySQL 建立连接的过程是比较耗时的。按照原来的方式建立一次连接只执行一条 SQL 的话，1s 只能执行 200 次数据库的查询，而数据库建立连接的时间占了其中 4/5。 解决方案也很简单，只要使用连接池将数据库连接预先建立好，这样在使用的时候就不需要频繁地创建连接了。调整之后，你发现 1s 就可以执行 1000 次的数据库查询，查询性能大大的提升了。\n数据库连接池有两个最重要的配置： 最小连接数和最大连接数 它们控制着从连接池中获取连接的流程：\n如果当前连接数小于最小连接数，则创建新的连接处理数据库请求；\n如果连接池中有空闲连接则复用空闲连接；\n如果空闲池中没有可以用的连接并且当前连接数小于最大连接数，则创建新的连接处理请求；\n如果当前连接数已经大于等于最大连接数，则按照配置中设定的时间（C3P0 的连接池配置是 checkoutTimeout）等待旧的连接可用；\n如果等待超过了这个设定时间则向用户抛出错误。\n连接池的维护问题 故障原因一：数据库的域名对应的 IP 发生了变更 故障原因二：wait_timeout参数，控制着当数据库连接闲置多长时间后，数据库会主动的关闭这条连接。这个机制对于数据库使用方是无感知的，所以当我们使用这个被关闭的连接时就会发生错误。\n怎么维护？\n启动一个线程来定期检测连接池中的连接是否可用，前 C3P0 连接池可以采用这种方式来检测连接是否可用。\n在获取到连接之后，先校验连接是否可用，如果可用才会执行 SQL 语句。比如 DBCP 连接池的 testOnBorrow 配置项，就是控制是否开启这个验证。这种方式在获取连接时会引入多余的开销， 在线上系统中还是尽量不要开启，在测试服务上可以使用。\n多线程并行处理与数据库之间的交互 用线程池预先创建线程 JDK 1.5 中引入的 ThreadPoolExecutor 就是一种线程池的实现，它有两个重要的参数：coreThreadCount 和 maxThreadCount，这两个参数控制着线程池的执行过程。\n如果线程池中的线程数少于 coreThreadCount 时，处理新的任务时会创建新的线程；\n如果线程数大于 coreThreadCount 则把任务丢到一个队列里面，由当前空闲的线程执行；\n当队列中的任务堆积满了的时候，则继续创建线程，直到达到 maxThreadCount；\n当线程数达到 maxTheadCount 时还有新的任务提交，那么我们就不得不将它们丢弃了\nJDK 实现的这个线程池优先把任务放入队列暂存起来，而不是创建更多的线程 ，它比较适用于执行 CPU 密集型的任务，也就是需要执行大量 CPU 运算的任务。这是为什么呢？因为执行 CPU 密集型的任务时 CPU 比较繁忙，因此只需要创建和 CPU 核数相当的线程就好了，多了反而会造成线程上下文切换，降低任务执行效率。所以当当前线程数超过核心线程数时，线程池不会增加线程，而是放在队列里等待核心线程空闲下来。\n我们平时开发的 Web 系统通常都有大量的 IO 操作，比方说查询数据库、查询缓存等等。任务在执行 IO 操作的时候 CPU 就空闲了下来，这时如果增加执行任务的线程数而不是把任务暂存在队列中，就可以在单位时间内执行更多的任务，大大提高了任务执行的吞吐量。所以你看 Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达 maxThreadCount，这样就比较适合于 Web 系统大量 IO 操作的场景了，你在实际运用过程中也可以参考借鉴。\n其次，线程池中使用的队列的堆积量也是我们需要监控的重要指标 ，对于实时性要求比较高的任务来说，这个指标尤为关键。\n遇到过任务被丢给线程池之后，长时间都没有被执行的诡异问题。是因为线程池的 coreThreadCount 和 maxThreadCount 设置的比较小，导致任务在线程池里面大量的堆积，在调大了这两个参数之后问题就解决了。\n最后， 如果你使用线程池请一定记住 不要使用无界队列（即没有设置固定大小的队列） 。大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会频繁地触发 Full GC，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个线程池使用了无界队列。\n回顾一下这两种技术，会发现它们都有一个 共同点： 它们所管理的对象，无论是连接还是线程，它们的创建过程都比较耗时，也比较消耗系统资源 。所以，我们把它们放在一个池子里统一管理起来，以达到 提升性能和资源复用的目的 。这是一种常见的软件设计思想，叫做池化技术， 它的核心思想是空间换时间，期望使用预先创建好的对象来减少频繁创建对象的性能开销，同时还可以对对象进行统一的管理，降低了对象的使用的成本，总之是好处多多。\n不过，池化技术也存在一些缺陷，比方说存储池子中的对象肯定需要消耗多余的内存，如果对象没有被频繁使用，就会造成内存上的浪费。再比方说，池子中的对象需要在系统启动的时候就预先创建完成，这在一定程度上增加了系统启动时间。 可这些缺陷相比池化技术的优势来说就比较微不足道了，只要我们确认要使用的对象在创建时确实比较耗时或者消耗资源，并且这些对象也确实会被频繁地创建和销毁，我们就可以使用池化技术来优化。\n池子的最大值和最小值的设置很重要，初期可以依据经验来设置，后面还是需要根据实际运行情况做调整。\n池子中的对象需要在使用之前预先初始化完成，这叫做 池子的预热 ，比方说使用线程池时就需要预先初始化所有的核心线程（在系统启动时提前创建和初始化资源）。如果池子未经过预热可能会导致系统重启后产生比较多的慢请求。\n池化技术核心是一种空间换时间优化方法的实践，所以要关注空间占用情况，避免出现空间过度使用出现内存泄露或者频繁垃圾回收等问题。\n主从分离 在 4 核 8G 的机器上运 MySQL 5.7 时，大概可以支撑 500 的 TPS 和 10000 的 QPS。\n查询增加时的主从读写分离 适用于读多写少的情况。原始的数据库我们称为 主库，主要负责数据的写入，拷贝的目标数据库称为 从库，主要负责支持数据查询。可以看到，主从读写分离有两个技术上的关键点：\n一个是数据的拷贝，我们称为主从复制； 在主从分离的情况下，我们 如何屏蔽主从分离带来的访问数据库方式的变化，让开发同学像是在使用单一数据库一样。 主从复制过程： 首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件中 而主库也会创建一个 log dump 线程来发送 binlog 给从库； 同时，从库还会创建一个 SQL 线程读取 relay log 中的内容，并且在从库中做回放，最终实现主从的一致性。这是一种比较常见的主从复制方式。 在这个方案中，使用独立的 log dump 线程是一种异步的方式，可以避免对主库的主体更新流程产生影响，而从库在接收到信息后并不是写入从库的存储中，是写入一个 relay log，是避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长。 主库的写入流程并没有等待主从同步完成就会返回结果，那么在极端的情况下，比如说主库上 binlog 还没有来得及刷新到磁盘上就出现了磁盘损坏或者机器掉电，就会导致 binlog 的丢失，最终造成主从数据的不一致。 不过，这种情况出现的概率很低，对于互联网的项目来说是可以容忍的。 随着从库数量增加，从库连接上来的 IO 线程比较多，主库也需要创建同样多的 log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，一般一个主库最多挂 3～5 个从库。\n主从复制的缺陷，部署复杂，还有一定的主从同步延迟。怎么解决延迟？这个问题解决的思路有很多，核心思想就是尽量不去从库中查询信息。\n数据冗余，发送消息队列时保留完整信息 使用缓存，在同步写数据库时，将数据写入缓存，获取数据时先读缓存 查询主库，要明确查询量级不会很大，在主库的承受范围 优先考虑第一种方案，缺点是单条消息比较大，从而增加消息发送的带宽和时间 缓存适合新增数据的场景，更新场景下可能造成数据不一致 不到万不得已不用第三种，因为这是一个主库查询的接口，难保其它人不滥用\n主从延迟导致短时间内读不到数据，过一段时间又可以了，需要把从库的落后时间作为重点数据指标监控和报警，正常时间是毫秒级，一旦达到秒级就需要告警。\n如何访问数据库 使用一个主库地址和多个从库地址，并且需要区分写入操作和查询操作。为了降低实现的复杂度，业界涌现了很多数据库中间件来解决数据库的访问问题，这些中间件可以分为两类。\n第一类以淘宝的 TDDL（ Taobao Distributed Data Layer），Sharding-JDBC为代表，以代码形式内嵌运行在应用程序内部。可以把它看成是一种数据源的代理，它的配置管理着多个数据源，每个数据源对应一个数据库，可能是主库，可能是从库。当有一个数据库请求时，中间件将 SQL 语句发给某一个指定的数据源来处理，然后将处理结果返回。 另一类是单独部署的代理层方案，这一类中间件部署在独立的服务器上（Mycat），业务代码如同在使用单一数据库一样使用它，实际上它内部管理着很多的数据源，当有数据库请求时，它会对 SQL 语句做必要的改写，然后发往指定的数据源。 写入增加时的分库分表 如何让数据库系统支持如此大的数据量呢？如何做到不同模块的故障隔离呢？数据库系统如何来处理更高的并发写入请求呢？ 对数据进行分片 ，对数据进行分片，可以很好地分摊数据库的读写压力，也可以突破单机的存储瓶颈，而常见的一种方式是对数据库做 分库分表 。 不同于主从复制时数据是全量地被拷贝到多个节点，分库分表后，每个节点只保存部分的数据，这样可以有效地减少单个数据库节点和单个数据表中存储的数据量，在解决了数据存储瓶颈的同时也能有效提升数据查询的性能 。同时，因为数据被分配到多个数据库节点上，那么数据的写入请求也从请求单一主库变成了请求多个数据分片节点，在一定程度上也会提升并发写入的性能。\n数据库分库分表的方式有两种：一种是垂直拆分，另一种是水平拆分。\n垂直拆分，顾名思义就是对数据库竖着拆分，也就是将数据库的 表拆分到多个不同的数据库中。\n垂直拆分的原则一般是按照业务类型来拆分，核心思想是专库专用，将业务耦合度比较高的表拆分到单独的库中。\n垂直拆分的关注点在于 业务相关性， 水平拆分指的是将单一数据表按照某一种规则拆分到多个数据库和多个数据表中，关注点在 数据的特点。\n水平拆分规则：\n哈希值拆分，比较适用于实体表，比如说用户表，内容表 按照某一个字段的 区间 来拆分，比较常用的是时间字段。 解决分库分表引入的问题 分库分表引入的一个最大的问题就是 引入了分库分表键，也叫做分区键， 也就是我们对数据库做分库分表所依据的字段。 无论是哈希拆分还是区间段的拆分，我们首先都需要选取一个数据库字段，这带来一个问题是：我们之后所有的查询都需要带上这个字段，才能找到数据所在的库和表，否则就只能向所有的数据库和数据表发送查询命令。 建立一个昵称和 ID 的映射表，在查询的时候要先通过昵称查询到 ID，再通过 ID 查询完整的数据，这个表也可以是分库分表的，也需要占用一定的存储空间，但是因为表中只有两个字段，所以相比重新做一次拆分还是会节省不少的空间的。\n分库分表引入的另外一个问题是一些数据库的特性在实现时可能变得很困难。 比如说多表的 join 在单库时是可以通过一个 SQL 语句完成的，但是拆分到多个数据库之后就无法跨库执行 SQL 了，不过好在我们对于 join 的需求不高，即使有也一般是把两个表的数据取出后在业务代码里面做筛选，复杂是有一些，不过是可以实现的。再比如说在未分库分表之前查询数据总数时只需要在 SQL 中执行 count() 即可，现在数据被分散到多个库表中，我们可能要考虑其他的方案，比方说将计数的数据单独存储在一张表中或者记录在 Redis 里面。\n如果在性能上没有瓶颈点那么就尽量不做分库分表；\n如果要做，就尽量一次到位，比如说 16 库 64 表就基本能够满足为了几年内你的业务的需求。\n很多的 NoSQL 数据库，例如 Hbase，MongoDB 都提供 auto sharding 的特性，如果你的团队内部对于这些组件比较熟悉，有较强的运维能力，那么也可以考虑使用这些 NoSQL 数据库替代传统的关系型数据库。\n数据库在分库分表之后，我们在使用数据库时存在的许多限制，比方说查询的时候必须带着分区键；一些聚合类的查询（像是 count()）性能较差，需要考虑使用计数器等其它的解决方案，其实分库分表还有一个问题就是 主键的全局唯一性的问题 。\nUnique ID Generator 数据库的主键选择 倾向于使用生成的 ID 作为数据库的主键\n基于 Snowflake 算法生成 ID有序，MySQL InnoDB 存储引擎使用 B+ 树 存储索引数据，而主键也是一种索引，ID 有序也会提升数据的写入性能，顺序写不需要寻道，会大大提升索引的写入性能 UUID的缺点，无序，不能作为 ID 的另一个原因是它不具备业务含义，UUID 是由 32 个 16 进制数字组成的字符串，如果作为数据库主键使用比较耗费空间\nSnowflake 算法如何把它工程化 一种是嵌入到业务代码里，也就是分布在业务服务器中，好处是业务代码在使用的时候不需要跨网络调用，性能上会好一些，但是就需要更多的机器 ID 位数来支持更多的业务服务器。另外，由于业务服务器的数量很多，我们很难保证机器 ID 的唯一性，所以就需要引入 ZooKeeper 等分布式一致性组件来保证每次机器重启时都能获得唯一的机器 ID。\n另外一个部署方式是作为独立的服务部署，这也就是我们常说的发号器服务。 业务在使用发号器的时候就需要多一次的网络调用，但是内网的调用对于性能的损耗有限，却可以减少机器 ID 的位数，如果发号器以主备方式部署，同时运行的只有一个发号器，那么机器 ID 可以省略，这样可以留更多的位数给最后的自增信息位。即使需要机器 ID，因为发号器部署实例数有限，那么就可以把机器 ID 写在发号器的配置文件里，这样即可以保证机器 ID 唯一性，也无需引入第三方组件了。 微博和美图都是使用独立服务的方式来部署发号器的，性能上单实例单 CPU 可以达到两万每秒。\nSnowflake 算法设计的非常简单且巧妙，性能上也足够高效，同时也能够生成 具有全局唯一性、单调递增性和有业务含义的 ID 。\n但是它也有一些缺点，其中最大的缺点就是它依赖于系统的时间戳，一旦系统时间不准，就有可能生成重复的 ID。 如果请求发号器的 QPS 不高，比如说发号器每毫秒只发一个 ID，就会造成生成 ID 的末位永远是 1，那么在分库分表时如果使用 ID 作为分区键就会造成库表分配的不均匀。\n解决办法主要有两个：\n时间戳不记录毫秒而是记录秒，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均。 生成的序列号的起始号可以做一下随机，这一秒是 21，下一秒是 30，这样就会尽量的均衡了。 这些改造： 一是要让算法中的 ID 生成规则符合自己业务的特点； 二是为了解决诸如时间回拨等问题。\nSnowflake 的算法并不复杂，你在使用的时候可以不考虑独立部署的问题，先想清楚按照自身的业务场景，需要如何设计 Snowflake 算法中的每一部分占的二进制位数。比如你的业务会部署几个 IDC，应用服务器要部署多少台机器，每秒钟发号个数的要求是多少等等，然后在业务代码中实现一个简单的版本先使用，等到应用服务器数量达到一定规模，再考虑独立部署的问题就可以了。这样可以避免多维护一套发号器服务，减少了运维上的复杂度。\n同一时间位，同一机器，在生成序列号时，是要上锁的吧？\n是的 不过像 redis 那样单线程处理就好了\nsnowflake 不能保证单调递增吧？首先，服务器的时钟可能有快有慢；其次，同一时刻，机器号大的机器生成的 ID 总是大于机器号小的机器，但他的请求可能是先到达了数据库。\n首先，服务器的时钟一般是对时的，其次，如果是单独部署的发号器，没有机器 ID 是可以保证单调递增的\n关于 41 位的时间戳的可支撑时间问题, 如果时间戳是从 0 开始计算则约可以支持 69 年, 但如果以当前时间开始算, 则可用的只有不到 20 年了( 69-(2019-1970) )\n实现时可以不以 1970 年为基准时间\nSQL和NoSQL互补 随机 IO 读写效率要比顺序 IO 小两到三个数量级 以 MySQL 的 InnoDB 存储引擎来说，更新 binlog、redolog、undolog 都是在做顺序 IO，而更新 datafile 和索引文件则是在做随机 IO，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后批量刷新到磁盘上，但是随机 IO 还是会发生。 索引在 InnoDB 引擎中是以 B+ 树（上一节课提到了 B+ 树，你可以回顾一下）方式来组织的，而 MySQL 主键是聚簇索引（一种索引类型，数据与索引数据放在一起），既然数据和索引数据放在一起，那么在数据插入或者更新的时候，我们需要找到要插入的位置，再把数据写到特定的位置上，这就产生了随机的 IO。而且一旦发生了页分裂，就不可避免会做数据的移动，也会极大地损耗写入性能。\nNoSQL 数据库怎么解决随机 IO的问题？ 最常见的方案，就是很多 NoSQL 数据库都在使用的 基于 LSM 树的存储引擎。\nLSM 树（Log-Structured Merge Tree）牺牲了一定的读性能来换取写入数据的高性能，Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。\n它的思想很简单，数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。\nMemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快。\n当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引。\n什么是倒排索引呢？ 倒排索引是指将记录中的某些列做分词，然后形成的分词与记录 ID 之间的映射关系。\nElasticsearch 作为一种常见的 NoSQL 数据库， 就以倒排索引作为核心技术原理，为你提供了分布式的全文搜索服务，这在传统的关系型数据库中使用 SQL 语句是很难实现的。 所以你看，NoSQL 可以在某些业务场景下代替传统数据库提供数据存储服务。\n使用 NoSQL 数据库彻底解决扩展性的问题 MongoDB 就有三个扩展性方面的特性\n其一是 Replica，也叫做副本集，你可以理解为主从分离，也就是通过将数据拷贝成多份来保证当主挂掉后数据不会丢失。同时呢，Replica 还可以分担读请求。Replica 中有主节点来承担写请求，并且把对数据变动记录到 oplog 里（类似于 binlog）；从节点接收到 oplog 后就会修改自身的数据以保持和主节点的一致。一旦主节点挂掉，MongoDB 会从从节点中选取一个节点成为主节点，可以继续提供写数据服务。\n其二是 Shard，也叫做分片，你可以理解为分库分表，即将数据按照某种规则拆分成多份，存储在不同的机器上。MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。\n其三是负载均衡，就是当 MongoDB 发现 Shard 之间数据分布不均匀，会启动 Balancer 进程对数据做重新的分配，最终让不同 Shard Server 的数据可以尽量的均衡。当我们的 Shard Server 存储空间不足需要扩容时，数据会自动被移动到新的 Shard Server 上，减少了数据迁移和验证的成本。\nNoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好补充。\nNoSQL 数据库在性能、扩展性上的优势：\n在性能方面，NoSQL 数据库使用一些算法将对磁盘的随机写转换成顺序写，提升了写的性能； 在某些场景下，比如全文搜索功能，关系型数据库并不能高效地支持，需要 NoSQL 数据库的支持 在扩展性方面，NoSQL 数据库天生支持分布式，支持数据冗余和数据分片的特性。 ","wordCount":"549","inLanguage":"en","image":"https://slowpeace2020.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-05-19T08:46:46-04:00","dateModified":"2024-05-19T08:46:46-04:00","author":[{"@type":"Person","name":"Slowpeace"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_1/"},"publisher":{"@type":"Organization","name":"slowpeace2020-blog","logo":{"@type":"ImageObject","url":"https://slowpeace2020.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://slowpeace2020.github.io/ accesskey=h title="slowpeace2020-blog (Alt + H)"><img src=https://slowpeace2020.github.io/apple-touch-icon.png alt aria-label=logo height=35>slowpeace2020-blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://slowpeace2020.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://slowpeace2020.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://slowpeace2020.github.io/ title=slowpeace2020><span>slowpeace2020</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://slowpeace2020.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://slowpeace2020.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">High_concurrent_distribute_system_design</h1><div class=post-description>High_concurrent_distribute_system_design introduction</div><div class=post-meta><span title='2024-05-19 08:46:46 -0400 -0400'>May 19, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;549 words&nbsp;·&nbsp;Slowpeace&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/High_concurrent_distribute_system_design_1.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#池化技术>池化技术</a><ul><li><a href=#连接池的维护问题>连接池的维护问题</a></li></ul></li><li><a href=#多线程并行处理与数据库之间的交互>多线程并行处理与数据库之间的交互</a><ul><li><a href=#用线程池预先创建线程>用线程池预先创建线程</a></li></ul></li><li><a href=#主从分离>主从分离</a><ul><li><a href=#查询增加时的主从读写分离>查询增加时的主从读写分离</a></li><li><a href=#如何访问数据库>如何访问数据库</a></li><li><a href=#写入增加时的分库分表>写入增加时的分库分表</a></li><li><a href=#解决分库分表引入的问题>解决分库分表引入的问题</a></li></ul></li><li><a href=#unique-id-generator>Unique ID Generator</a><ul><li><a href=#数据库的主键选择>数据库的主键选择</a></li><li><a href=#基于-snowflake-算法生成>基于 Snowflake 算法生成</a></li></ul></li><li><a href=#sql和nosql互补>SQL和NoSQL互补</a><ul><li><a href=#nosql-数据库怎么解决随机-io的问题>NoSQL 数据库怎么解决随机 IO的问题？</a></li><li><a href=#什么是倒排索引呢>什么是倒排索引呢？</a></li><li><a href=#使用-nosql-数据库彻底解决扩展性的问题>使用 NoSQL 数据库彻底解决扩展性的问题</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=演进篇之数据库>演进篇之数据库<a hidden class=anchor aria-hidden=true href=#演进篇之数据库>#</a></h1><h2 id=池化技术>池化技术<a hidden class=anchor aria-hidden=true href=#池化技术>#</a></h2><p>刚开始数据库的调用方式是先获取数据库的连接，然后依靠这条连接从数据库中查询数据，最后关闭连接释放数据库资源。频繁地建立数据库连接耗费时间长导致了访问慢的问题。整个 MySQL 的连接过程大概消耗了 4ms，SQL 的平均执行时间大概是 1ms，也就是说相比于 SQL 的执行，MySQL 建立连接的过程是比较耗时的。按照原来的方式建立一次连接只执行一条 SQL 的话，1s 只能执行 200 次数据库的查询，而数据库建立连接的时间占了其中 4/5。
解决方案也很简单，只要使用连接池将数据库连接预先建立好，这样在使用的时候就不需要频繁地创建连接了。调整之后，你发现 1s 就可以执行 1000 次的数据库查询，查询性能大大的提升了。</p><p>数据库连接池有两个最重要的配置： 最小连接数和最大连接数
它们控制着从连接池中获取连接的流程：</p><p>如果当前连接数小于最小连接数，则创建新的连接处理数据库请求；<br>如果连接池中有空闲连接则复用空闲连接；<br>如果空闲池中没有可以用的连接并且当前连接数小于最大连接数，则创建新的连接处理请求；<br>如果当前连接数已经大于等于最大连接数，则按照配置中设定的时间（C3P0 的连接池配置是 checkoutTimeout）等待旧的连接可用；<br>如果等待超过了这个设定时间则向用户抛出错误。</p><h3 id=连接池的维护问题>连接池的维护问题<a hidden class=anchor aria-hidden=true href=#连接池的维护问题>#</a></h3><p>故障原因一：数据库的域名对应的 IP 发生了变更
故障原因二：wait_timeout参数，控制着当数据库连接闲置多长时间后，数据库会主动的关闭这条连接。这个机制对于数据库使用方是无感知的，所以当我们使用这个被关闭的连接时就会发生错误。</p><p>怎么维护？<br>启动一个线程来定期检测连接池中的连接是否可用，前 C3P0 连接池可以采用这种方式来检测连接是否可用。<br>在获取到连接之后，先校验连接是否可用，如果可用才会执行 SQL 语句。比如 DBCP 连接池的 testOnBorrow 配置项，就是控制是否开启这个验证。这种方式在获取连接时会引入多余的开销， 在线上系统中还是尽量不要开启，在测试服务上可以使用。</p><h2 id=多线程并行处理与数据库之间的交互>多线程并行处理与数据库之间的交互<a hidden class=anchor aria-hidden=true href=#多线程并行处理与数据库之间的交互>#</a></h2><h3 id=用线程池预先创建线程>用线程池预先创建线程<a hidden class=anchor aria-hidden=true href=#用线程池预先创建线程>#</a></h3><p>JDK 1.5 中引入的 ThreadPoolExecutor 就是一种线程池的实现，它有两个重要的参数：coreThreadCount 和 maxThreadCount，这两个参数控制着线程池的执行过程。<br>如果线程池中的线程数少于 coreThreadCount 时，处理新的任务时会创建新的线程；<br>如果线程数大于 coreThreadCount 则把任务丢到一个队列里面，由当前空闲的线程执行；<br>当队列中的任务堆积满了的时候，则继续创建线程，直到达到 maxThreadCount；<br>当线程数达到 maxTheadCount 时还有新的任务提交，那么我们就不得不将它们丢弃了</p><p>JDK 实现的这个线程池优先把任务放入队列暂存起来，而不是创建更多的线程 ，它比较适用于执行 CPU 密集型的任务，也就是需要执行大量 CPU 运算的任务。这是为什么呢？因为执行 CPU 密集型的任务时 CPU 比较繁忙，因此只需要创建和 CPU 核数相当的线程就好了，多了反而会造成线程上下文切换，降低任务执行效率。所以当当前线程数超过核心线程数时，线程池不会增加线程，而是放在队列里等待核心线程空闲下来。</p><p>我们平时开发的 Web 系统通常都有大量的 IO 操作，比方说查询数据库、查询缓存等等。任务在执行 IO 操作的时候 CPU 就空闲了下来，这时如果增加执行任务的线程数而不是把任务暂存在队列中，就可以在单位时间内执行更多的任务，大大提高了任务执行的吞吐量。所以你看 Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达 maxThreadCount，这样就比较适合于 Web 系统大量 IO 操作的场景了，你在实际运用过程中也可以参考借鉴。</p><p>其次，线程池中使用的队列的堆积量也是我们需要监控的重要指标 ，对于实时性要求比较高的任务来说，这个指标尤为关键。</p><p>遇到过任务被丢给线程池之后，长时间都没有被执行的诡异问题。是因为线程池的 coreThreadCount 和 maxThreadCount 设置的比较小，导致任务在线程池里面大量的堆积，在调大了这两个参数之后问题就解决了。</p><p>最后， 如果你使用线程池请一定记住 不要使用无界队列（即没有设置固定大小的队列） 。大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会频繁地触发 Full GC，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个线程池使用了无界队列。</p><p>回顾一下这两种技术，会发现它们都有一个 共同点： 它们所管理的对象，无论是连接还是线程，它们的创建过程都比较耗时，也比较消耗系统资源 。所以，我们把它们放在一个池子里统一管理起来，以达到 提升性能和资源复用的目的 。这是一种常见的软件设计思想，叫做池化技术， 它的核心思想是空间换时间，期望使用预先创建好的对象来减少频繁创建对象的性能开销，同时还可以对对象进行统一的管理，降低了对象的使用的成本，总之是好处多多。</p><p>不过，池化技术也存在一些缺陷，比方说存储池子中的对象肯定需要消耗多余的内存，如果对象没有被频繁使用，就会造成内存上的浪费。再比方说，池子中的对象需要在系统启动的时候就预先创建完成，这在一定程度上增加了系统启动时间。 可这些缺陷相比池化技术的优势来说就比较微不足道了，只要我们确认要使用的对象在创建时确实比较耗时或者消耗资源，并且这些对象也确实会被频繁地创建和销毁，我们就可以使用池化技术来优化。</p><p>池子的最大值和最小值的设置很重要，初期可以依据经验来设置，后面还是需要根据实际运行情况做调整。<br>池子中的对象需要在使用之前预先初始化完成，这叫做 池子的预热 ，比方说使用线程池时就需要预先初始化所有的核心线程（在系统启动时提前创建和初始化资源）。如果池子未经过预热可能会导致系统重启后产生比较多的慢请求。<br>池化技术核心是一种空间换时间优化方法的实践，所以要关注空间占用情况，避免出现空间过度使用出现内存泄露或者频繁垃圾回收等问题。</p><h2 id=主从分离>主从分离<a hidden class=anchor aria-hidden=true href=#主从分离>#</a></h2><p>在 4 核 8G 的机器上运 MySQL 5.7 时，大概可以支撑 500 的 TPS 和 10000 的 QPS。</p><h3 id=查询增加时的主从读写分离>查询增加时的主从读写分离<a hidden class=anchor aria-hidden=true href=#查询增加时的主从读写分离>#</a></h3><p>适用于读多写少的情况。原始的数据库我们称为 主库，主要负责数据的写入，拷贝的目标数据库称为 从库，主要负责支持数据查询。可以看到，主从读写分离有两个技术上的关键点：</p><ol><li>一个是数据的拷贝，我们称为主从复制；</li><li>在主从分离的情况下，我们 如何屏蔽主从分离带来的访问数据库方式的变化，让开发同学像是在使用单一数据库一样。</li></ol><p>主从复制过程：
首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件中
而主库也会创建一个 log dump 线程来发送 binlog 给从库；
同时，从库还会创建一个 SQL 线程读取 relay log 中的内容，并且在从库中做回放，最终实现主从的一致性。这是一种比较常见的主从复制方式。
在这个方案中，使用独立的 log dump 线程是一种异步的方式，可以避免对主库的主体更新流程产生影响，而从库在接收到信息后并不是写入从库的存储中，是写入一个 relay log，是避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长。
<img loading=lazy src=../../assets/images/master-slave.png alt=主从复制过程></p><p>主库的写入流程并没有等待主从同步完成就会返回结果，那么在极端的情况下，比如说主库上 binlog 还没有来得及刷新到磁盘上就出现了磁盘损坏或者机器掉电，就会导致 binlog 的丢失，最终造成主从数据的不一致。 不过，这种情况出现的概率很低，对于互联网的项目来说是可以容忍的。
随着从库数量增加，从库连接上来的 IO 线程比较多，主库也需要创建同样多的 log dump 线程来处理复制的请求，对于主库资源消耗比较高，同时受限于主库的网络带宽，所以在实际使用中，一般一个主库最多挂 3～5 个从库。</p><p>主从复制的缺陷，部署复杂，还有一定的主从同步延迟。怎么解决延迟？这个问题解决的思路有很多，核心思想就是尽量不去从库中查询信息。</p><ol><li>数据冗余，发送消息队列时保留完整信息</li><li>使用缓存，在同步写数据库时，将数据写入缓存，获取数据时先读缓存</li><li>查询主库，要明确查询量级不会很大，在主库的承受范围</li></ol><p>优先考虑第一种方案，缺点是单条消息比较大，从而增加消息发送的带宽和时间
缓存适合新增数据的场景，更新场景下可能造成数据不一致
不到万不得已不用第三种，因为这是一个主库查询的接口，难保其它人不滥用</p><p>主从延迟导致短时间内读不到数据，过一段时间又可以了，需要把从库的落后时间作为重点数据指标监控和报警，正常时间是毫秒级，一旦达到秒级就需要告警。</p><h3 id=如何访问数据库>如何访问数据库<a hidden class=anchor aria-hidden=true href=#如何访问数据库>#</a></h3><p>使用一个主库地址和多个从库地址，并且需要区分写入操作和查询操作。为了降低实现的复杂度，业界涌现了很多数据库中间件来解决数据库的访问问题，这些中间件可以分为两类。</p><ol><li>第一类以淘宝的 TDDL（ Taobao Distributed Data Layer），Sharding-JDBC为代表，以代码形式内嵌运行在应用程序内部。可以把它看成是一种数据源的代理，它的配置管理着多个数据源，每个数据源对应一个数据库，可能是主库，可能是从库。当有一个数据库请求时，中间件将 SQL 语句发给某一个指定的数据源来处理，然后将处理结果返回。</li><li>另一类是单独部署的代理层方案，这一类中间件部署在独立的服务器上（Mycat），业务代码如同在使用单一数据库一样使用它，实际上它内部管理着很多的数据源，当有数据库请求时，它会对 SQL 语句做必要的改写，然后发往指定的数据源。</li></ol><h3 id=写入增加时的分库分表>写入增加时的分库分表<a hidden class=anchor aria-hidden=true href=#写入增加时的分库分表>#</a></h3><p>如何让数据库系统支持如此大的数据量呢？如何做到不同模块的故障隔离呢？数据库系统如何来处理更高的并发写入请求呢？
对数据进行分片 ，对数据进行分片，可以很好地分摊数据库的读写压力，也可以突破单机的存储瓶颈，而常见的一种方式是对数据库做 分库分表 。
不同于主从复制时数据是全量地被拷贝到多个节点，分库分表后，每个节点只保存部分的数据，这样可以有效地减少单个数据库节点和单个数据表中存储的数据量，在解决了数据存储瓶颈的同时也能有效提升数据查询的性能 。同时，因为数据被分配到多个数据库节点上，那么数据的写入请求也从请求单一主库变成了请求多个数据分片节点，在一定程度上也会提升并发写入的性能。</p><p>数据库分库分表的方式有两种：一种是垂直拆分，另一种是水平拆分。</p><p>垂直拆分，顾名思义就是对数据库竖着拆分，也就是将数据库的 表拆分到多个不同的数据库中。</p><p>垂直拆分的原则一般是按照业务类型来拆分，核心思想是专库专用，将业务耦合度比较高的表拆分到单独的库中。</p><p>垂直拆分的关注点在于 业务相关性，
水平拆分指的是将单一数据表按照某一种规则拆分到多个数据库和多个数据表中，关注点在 数据的特点。</p><p>水平拆分规则：</p><ol><li>哈希值拆分，比较适用于实体表，比如说用户表，内容表</li><li>按照某一个字段的 区间 来拆分，比较常用的是时间字段。</li></ol><h3 id=解决分库分表引入的问题>解决分库分表引入的问题<a hidden class=anchor aria-hidden=true href=#解决分库分表引入的问题>#</a></h3><p>分库分表引入的一个最大的问题就是 引入了分库分表键，也叫做分区键， 也就是我们对数据库做分库分表所依据的字段。
无论是哈希拆分还是区间段的拆分，我们首先都需要选取一个数据库字段，这带来一个问题是：我们之后所有的查询都需要带上这个字段，才能找到数据所在的库和表，否则就只能向所有的数据库和数据表发送查询命令。
建立一个昵称和 ID 的映射表，在查询的时候要先通过昵称查询到 ID，再通过 ID 查询完整的数据，这个表也可以是分库分表的，也需要占用一定的存储空间，但是因为表中只有两个字段，所以相比重新做一次拆分还是会节省不少的空间的。</p><p>分库分表引入的另外一个问题是一些数据库的特性在实现时可能变得很困难。 比如说多表的 join 在单库时是可以通过一个 SQL 语句完成的，但是拆分到多个数据库之后就无法跨库执行 SQL 了，不过好在我们对于 join 的需求不高，即使有也一般是把两个表的数据取出后在业务代码里面做筛选，复杂是有一些，不过是可以实现的。再比如说在未分库分表之前查询数据总数时只需要在 SQL 中执行 count() 即可，现在数据被分散到多个库表中，我们可能要考虑其他的方案，比方说将计数的数据单独存储在一张表中或者记录在 Redis 里面。</p><p>如果在性能上没有瓶颈点那么就尽量不做分库分表；</p><p>如果要做，就尽量一次到位，比如说 16 库 64 表就基本能够满足为了几年内你的业务的需求。</p><p>很多的 NoSQL 数据库，例如 Hbase，MongoDB 都提供 auto sharding 的特性，如果你的团队内部对于这些组件比较熟悉，有较强的运维能力，那么也可以考虑使用这些 NoSQL 数据库替代传统的关系型数据库。</p><p>数据库在分库分表之后，我们在使用数据库时存在的许多限制，比方说查询的时候必须带着分区键；一些聚合类的查询（像是 count()）性能较差，需要考虑使用计数器等其它的解决方案，其实分库分表还有一个问题就是 主键的全局唯一性的问题 。</p><h2 id=unique-id-generator>Unique ID Generator<a hidden class=anchor aria-hidden=true href=#unique-id-generator>#</a></h2><h3 id=数据库的主键选择>数据库的主键选择<a hidden class=anchor aria-hidden=true href=#数据库的主键选择>#</a></h3><p>倾向于使用生成的 ID 作为数据库的主键</p><h3 id=基于-snowflake-算法生成>基于 Snowflake 算法生成<a hidden class=anchor aria-hidden=true href=#基于-snowflake-算法生成>#</a></h3><p>ID有序，MySQL InnoDB 存储引擎使用 B+ 树 存储索引数据，而主键也是一种索引，ID 有序也会提升数据的写入性能，顺序写不需要寻道，会大大提升索引的写入性能
UUID的缺点，无序，不能作为 ID 的另一个原因是它不具备业务含义，UUID 是由 32 个 16 进制数字组成的字符串，如果作为数据库主键使用比较耗费空间</p><h4 id=snowflake-算法如何把它工程化>Snowflake 算法如何把它工程化<a hidden class=anchor aria-hidden=true href=#snowflake-算法如何把它工程化>#</a></h4><p>一种是嵌入到业务代码里，也就是分布在业务服务器中，好处是业务代码在使用的时候不需要跨网络调用，性能上会好一些，但是就需要更多的机器 ID 位数来支持更多的业务服务器。另外，由于业务服务器的数量很多，我们很难保证机器 ID 的唯一性，所以就需要引入 ZooKeeper 等分布式一致性组件来保证每次机器重启时都能获得唯一的机器 ID。</p><p>另外一个部署方式是作为独立的服务部署，这也就是我们常说的发号器服务。 业务在使用发号器的时候就需要多一次的网络调用，但是内网的调用对于性能的损耗有限，却可以减少机器 ID 的位数，如果发号器以主备方式部署，同时运行的只有一个发号器，那么机器 ID 可以省略，这样可以留更多的位数给最后的自增信息位。即使需要机器 ID，因为发号器部署实例数有限，那么就可以把机器 ID 写在发号器的配置文件里，这样即可以保证机器 ID 唯一性，也无需引入第三方组件了。 微博和美图都是使用独立服务的方式来部署发号器的，性能上单实例单 CPU 可以达到两万每秒。</p><p>Snowflake 算法设计的非常简单且巧妙，性能上也足够高效，同时也能够生成 具有全局唯一性、单调递增性和有业务含义的 ID 。<br>但是它也有一些缺点，其中最大的缺点就是它依赖于系统的时间戳，一旦系统时间不准，就有可能生成重复的 ID。
如果请求发号器的 QPS 不高，比如说发号器每毫秒只发一个 ID，就会造成生成 ID 的末位永远是 1，那么在分库分表时如果使用 ID 作为分区键就会造成库表分配的不均匀。</p><p>解决办法主要有两个：</p><ol><li>时间戳不记录毫秒而是记录秒，这样在一个时间区间里可以多发出几个号，避免出现分库分表时数据分配不均。</li><li>生成的序列号的起始号可以做一下随机，这一秒是 21，下一秒是 30，这样就会尽量的均衡了。</li></ol><p>这些改造： 一是要让算法中的 ID 生成规则符合自己业务的特点； 二是为了解决诸如时间回拨等问题。</p><p>Snowflake 的算法并不复杂，你在使用的时候可以不考虑独立部署的问题，先想清楚按照自身的业务场景，需要如何设计 Snowflake 算法中的每一部分占的二进制位数。比如你的业务会部署几个 IDC，应用服务器要部署多少台机器，每秒钟发号个数的要求是多少等等，然后在业务代码中实现一个简单的版本先使用，等到应用服务器数量达到一定规模，再考虑独立部署的问题就可以了。这样可以避免多维护一套发号器服务，减少了运维上的复杂度。</p><p>同一时间位，同一机器，在生成序列号时，是要上锁的吧？</p><p>是的 不过像 redis 那样单线程处理就好了</p><p>snowflake 不能保证单调递增吧？首先，服务器的时钟可能有快有慢；其次，同一时刻，机器号大的机器生成的 ID 总是大于机器号小的机器，但他的请求可能是先到达了数据库。</p><p>首先，服务器的时钟一般是对时的，其次，如果是单独部署的发号器，没有机器 ID 是可以保证单调递增的</p><p>关于 41 位的时间戳的可支撑时间问题, 如果时间戳是从 0 开始计算则约可以支持 69 年, 但如果以当前时间开始算, 则可用的只有不到 20 年了( 69-(2019-1970) )</p><p>实现时可以不以 1970 年为基准时间</p><h2 id=sql和nosql互补>SQL和NoSQL互补<a hidden class=anchor aria-hidden=true href=#sql和nosql互补>#</a></h2><p>随机 IO 读写效率要比顺序 IO 小两到三个数量级
以 MySQL 的 InnoDB 存储引擎来说，更新 binlog、redolog、undolog 都是在做顺序 IO，而更新 datafile 和索引文件则是在做随机 IO，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后批量刷新到磁盘上，但是随机 IO 还是会发生。
索引在 InnoDB 引擎中是以 B+ 树（上一节课提到了 B+ 树，你可以回顾一下）方式来组织的，而 MySQL 主键是聚簇索引（一种索引类型，数据与索引数据放在一起），既然数据和索引数据放在一起，那么在数据插入或者更新的时候，我们需要找到要插入的位置，再把数据写到特定的位置上，这就产生了随机的 IO。而且一旦发生了页分裂，就不可避免会做数据的移动，也会极大地损耗写入性能。</p><h3 id=nosql-数据库怎么解决随机-io的问题>NoSQL 数据库怎么解决随机 IO的问题？<a hidden class=anchor aria-hidden=true href=#nosql-数据库怎么解决随机-io的问题>#</a></h3><p>最常见的方案，就是很多 NoSQL 数据库都在使用的 基于 LSM 树的存储引擎。</p><p>LSM 树（Log-Structured Merge Tree）牺牲了一定的读性能来换取写入数据的高性能，Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。</p><p>它的思想很简单，数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。</p><p>MemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快。</p><p>当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引。</p><h3 id=什么是倒排索引呢>什么是倒排索引呢？<a hidden class=anchor aria-hidden=true href=#什么是倒排索引呢>#</a></h3><p>倒排索引是指将记录中的某些列做分词，然后形成的分词与记录 ID 之间的映射关系。</p><p>Elasticsearch 作为一种常见的 NoSQL 数据库， 就以倒排索引作为核心技术原理，为你提供了分布式的全文搜索服务，这在传统的关系型数据库中使用 SQL 语句是很难实现的。 所以你看，NoSQL 可以在某些业务场景下代替传统数据库提供数据存储服务。</p><h3 id=使用-nosql-数据库彻底解决扩展性的问题>使用 NoSQL 数据库彻底解决扩展性的问题<a hidden class=anchor aria-hidden=true href=#使用-nosql-数据库彻底解决扩展性的问题>#</a></h3><p>MongoDB 就有三个扩展性方面的特性</p><p>其一是 Replica，也叫做副本集，你可以理解为主从分离，也就是通过将数据拷贝成多份来保证当主挂掉后数据不会丢失。同时呢，Replica 还可以分担读请求。Replica 中有主节点来承担写请求，并且把对数据变动记录到 oplog 里（类似于 binlog）；从节点接收到 oplog 后就会修改自身的数据以保持和主节点的一致。一旦主节点挂掉，MongoDB 会从从节点中选取一个节点成为主节点，可以继续提供写数据服务。</p><p>其二是 Shard，也叫做分片，你可以理解为分库分表，即将数据按照某种规则拆分成多份，存储在不同的机器上。MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。</p><p>其三是负载均衡，就是当 MongoDB 发现 Shard 之间数据分布不均匀，会启动 Balancer 进程对数据做重新的分配，最终让不同 Shard Server 的数据可以尽量的均衡。当我们的 Shard Server 存储空间不足需要扩容时，数据会自动被移动到新的 Shard Server 上，减少了数据迁移和验证的成本。</p><p>NoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好补充。</p><p>NoSQL 数据库在性能、扩展性上的优势：</p><ol><li>在性能方面，NoSQL 数据库使用一些算法将对磁盘的随机写转换成顺序写，提升了写的性能；</li><li>在某些场景下，比如全文搜索功能，关系型数据库并不能高效地支持，需要 NoSQL 数据库的支持</li><li>在扩展性方面，NoSQL 数据库天生支持分布式，支持数据冗余和数据分片的特性。</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://slowpeace2020.github.io/tags/system-design/>System Design</a></li><li><a href=https://slowpeace2020.github.io/tags/distribute/>Distribute</a></li><li><a href=https://slowpeace2020.github.io/tags/concurrent/>Concurrent</a></li></ul><nav class=paginav><a class=prev href=https://slowpeace2020.github.io/posts/high_concurrent_distribute_system_design_2/><span class=title>« Prev</span><br><span>High_concurrent_distribute_system_design</span>
</a><a class=next href=https://slowpeace2020.github.io/posts/interview_note/><span class=title>Next »</span><br><span>High_concurrent_distribute_system_design</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on x" href="https://x.com/intent/tweet/?text=High_concurrent_distribute_system_design&amp;url=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f&amp;hashtags=systemdesign%2cdistribute%2cconcurrent"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f&amp;title=High_concurrent_distribute_system_design&amp;summary=High_concurrent_distribute_system_design&amp;source=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f&title=High_concurrent_distribute_system_design"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on whatsapp" href="https://api.whatsapp.com/send?text=High_concurrent_distribute_system_design%20-%20https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on telegram" href="https://telegram.me/share/url?text=High_concurrent_distribute_system_design&amp;url=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share High_concurrent_distribute_system_design on ycombinator" href="https://news.ycombinator.com/submitlink?t=High_concurrent_distribute_system_design&u=https%3a%2f%2fslowpeace2020.github.io%2fposts%2fhigh_concurrent_distribute_system_design_1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://slowpeace2020.github.io/>slowpeace2020-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>